library(keras)
library(keras)
lines = readr::read_lines('cleaned_186.txt', skip_empty_rows = T)
lines = readr::read_lines('cleaned186.txt', skip_empty_rows = T)
lines
lines[1]
as.character(lines[1])
lines = readr::read_lines('cleaned186.txt', skip_empty_rows = T, locale = readr::locale(encoding = "utf8"))
lines = readr::read_lines('cleaned186.txt', skip_empty_rows = T, locale = readr::locale(encoding = "UTF-8"))
lines
lines = readr::read_lines('cleaned186.txt', skip_empty_rows = T, locale = readr::locale(encoding = "WINDOWS-1251"))
lines
## initialize tokenizer and fit
tokenizer <- text_tokenizer() %>%
fit_text_tokenizer(sov_questions)
lines[1:10]
lines = readr::read_lines('cleaned186.txt',
#skip_empty_rows = T,
locale = readr::locale(encoding = "WINDOWS-1251"))
lines[1:10]
##########
lines = lines[nchar(lines)>0]
## initialize tokenizer and fit
tokenizer <- text_tokenizer() %>%
fit_text_tokenizer(lines)
## inspect word_index
tokenizer$word_index
lines
## modes
texts_to_matrix(tokenizer, lines, mode = "binary")
texts_to_matrix(tokenizer, lines, mode = "tfidf")
texts_to_sequences(tokenizer, lines)
sequences = texts_to_sequences(tokenizer, lines)
sequences = pad_sequences(sequences, maxlen = 50)
View(sequences)
View(sequences)
sequences = texts_to_sequences(tokenizer, lines)
sequences = pad_sequences(sequences, maxlen = 10)
save_text_tokenizer(tokenizer, 'tokenizer')
